#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Balad.ir Supermarket Scraper
Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø´Ù‡Ø± Ø§Ø² Ø³Ø§ÛŒØª Ø¨Ù„Ø¯
"""

import requests
import json
import time
from typing import List, Dict, Optional
import sys
import os

class BaladSupermarketScraper:
    def __init__(self, city: str = "ØªÙ‡Ø±Ø§Ù†", delay: float = 1.0):
        """
        Initialize scraper for a specific city
        
        Args:
            city: Ù†Ø§Ù… Ø´Ù‡Ø± Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ (Ù…Ø«Ù„Ø§Ù‹: ØªÙ‡Ø±Ø§Ù†ØŒ Ø§ØµÙÙ‡Ø§Ù†ØŒ Ù…Ø´Ù‡Ø¯)
            delay: ØªØ£Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)
        """
        self.city = city
        self.delay = delay
        self.base_url = f"https://balad.ir/city-{city}/cat-supermarket"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fa-IR,fa;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        self.all_supermarkets = []
    
    def fetch_page(self, page_number: int = 1) -> Optional[Dict]:
        """
        Fetch a single page of supermarket data
        
        Args:
            page_number: Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡
            
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØµÙØ­Ù‡ ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        """
        try:
            url = f"{self.base_url}?page={page_number}" if page_number > 1 else self.base_url
            
            print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ {page_number}...")
            
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON Ø§Ø² HTML
            import re
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø±ÙˆØ´ Û±: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± __NEXT_DATA__
            next_data_script = soup.find('script', {'id': '__NEXT_DATA__'})
            
            if next_data_script:
                data = json.loads(next_data_script.string)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡
                items = self._extract_items_from_data(data)
                
                if items:
                    print(f"âœ… ØµÙØ­Ù‡ {page_number}: {len(items)} Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª ÛŒØ§ÙØª Ø´Ø¯")
                    return {
                        'items': items,
                        'page_number': page_number,
                        'has_next': self._has_next_page(data, page_number)
                    }
            
            # Ø±ÙˆØ´ Û²: Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± JSON-LD
            json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})
            
            for script in json_ld_scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, list) and len(data) > 0 and 'name' in data[0]:
                        items = self._extract_from_json_ld(data)
                        if items:
                            print(f"âœ… ØµÙØ­Ù‡ {page_number}: {len(items)} Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª ÛŒØ§ÙØª Ø´Ø¯")
                            return {
                                'items': items,
                                'page_number': page_number,
                                'has_next': True  # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
                            }
                except:
                    continue
            
            print(f"âš ï¸  ØµÙØ­Ù‡ {page_number}: Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return None
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ {page_number}: {e}")
            return None
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± ØµÙØ­Ù‡ {page_number}: {e}")
            return None
    
    def _extract_items_from_data(self, data: Dict) -> List[Dict]:
        """
        Extract supermarket items from the data structure
        """
        items = []
        
        try:
            # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§
            paths_to_try = [
                # Ù…Ø³ÛŒØ± Ø§ØµÙ„ÛŒ Ø¯Ø± Balad
                ['props', 'pageProps', 'data', 'items'],
                # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
                ['items'],
                ['data', 'items'],
                ['result', 'items']
            ]
            
            for path in paths_to_try:
                current = data
                found = True
                
                for key in path:
                    if isinstance(current, dict) and key in current:
                        current = current[key]
                    else:
                        found = False
                        break
                
                if found and isinstance(current, list):
                    items = current
                    break
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
            standardized_items = []
            for item in items:
                standardized = self._standardize_item(item)
                if standardized:
                    standardized_items.append(standardized)
            
            return standardized_items
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§: {e}")
            return []
    
    def _extract_from_json_ld(self, data: List[Dict]) -> List[Dict]:
        """
        Extract from JSON-LD structured data
        """
        items = []
        
        for item in data:
            try:
                standardized = {
                    'name': item.get('name', ''),
                    'telephone': item.get('telephone', ''),
                    'address': item.get('address', ''),
                    'location': item.get('geo', {})
                }
                
                # ÙÙ‚Ø· Ø§Ú¯Ø± Ù†Ø§Ù… Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                if standardized['name']:
                    items.append(standardized)
            except:
                continue
        
        return items
    
    def _standardize_item(self, item: Dict) -> Optional[Dict]:
        """
        Convert raw item to standardized format
        """
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…
            name = item.get('name', '')
            if not name:
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ„ÙÙ†
            telephone = item.get('telephone', '')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø¯Ø±Ø³
            address = item.get('address', '')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ú©Ø§Ù†ÛŒ
            location = {'lat': None, 'lon': None}
            
            # Ø¨Ø±Ø±Ø³ÛŒ geometry Ø¨Ø±Ø§ÛŒ Ù…Ø®ØªØµØ§Øª
            geometry = item.get('geometry', {})
            if geometry and 'coordinates' in geometry:
                coords = geometry['coordinates']
                if len(coords) >= 2:
                    # ÙØ±Ù…Øª: [longitude, latitude]
                    location['lon'] = coords[0]
                    location['lat'] = coords[1]
            
            # Ø§Ú¯Ø± geometry Ù†Ø¨ÙˆØ¯ØŒ geo Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†
            if location['lat'] is None:
                geo = item.get('geo', {})
                if geo:
                    location['lat'] = geo.get('latitude')
                    location['lon'] = geo.get('longitude')
            
            # Ø³Ø§Ø®Øª Ø¢ÛŒØªÙ… Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
            standardized = {
                'name': name,
                'phone': telephone,
                'address': address,
                'location': location
            }
            
            # Ø­Ø°Ù ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ
            return {k: v for k, v in standardized.items() if v not in [None, '', {}]}
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¢ÛŒØªÙ…: {e}")
            return None
    
    def _has_next_page(self, data: Dict, current_page: int) -> bool:
        """
        Check if there are more pages
        """
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ pageCount
            page_count = data.get('props', {}).get('pageProps', {}).get('data', {}).get('pageCount', 0)
            if page_count > current_page:
                return True
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§
            items = self._extract_items_from_data(data)
            if items and len(items) > 0:
                return True
            
            return False
            
        except:
            # Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ±Ø¶ Ú©Ù† ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            return True
    
    def scrape_all_pages(self, max_pages: int = 100) -> List[Dict]:
        """
        Scrape all pages of supermarkets
        
        Args:
            max_pages: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾
            
        Returns:
            Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§
        """
        print(f"\nğŸ™ï¸  Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ {self.city}")
        print("=" * 50)
        
        page_number = 1
        total_items = 0
        
        while page_number <= max_pages:
            # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡
            page_data = self.fetch_page(page_number)
            
            if not page_data or not page_data['items']:
                print(f"\nâ¹ï¸  ØµÙØ­Ù‡ {page_number} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. ØªÙˆÙ‚Ù Ø§Ø³Ú©Ø±Ù¾.")
                break
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§
            new_items = page_data['items']
            self.all_supermarkets.extend(new_items)
            total_items += len(new_items)
            
            print(f"ğŸ“Š ØªØ§Ú©Ù†ÙˆÙ†: {total_items} Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡")
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¯Ø§Ù…Ù‡â€ŒØ¯Ø§Ø± Ø¨ÙˆØ¯Ù†
            if not page_data.get('has_next', True):
                print(f"\nâœ… Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† ØµÙØ­Ù‡ Ø±Ø³ÛŒØ¯ÛŒÙ….")
                break
            
            # ØªØ£Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
            time.sleep(self.delay)
            
            # ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ
            page_number += 1
        
        print(f"\nğŸ‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ {self.city}: {len(self.all_supermarkets)}")
        
        return self.all_supermarkets
    
    def remove_duplicates(self) -> List[Dict]:
        """
        Remove duplicate supermarkets based on name and address
        """
        unique_supermarkets = []
        seen = set()
        
        for supermarket in self.all_supermarkets:
            # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø§Ø² Ù†Ø§Ù… Ùˆ Ø¢Ø¯Ø±Ø³
            key = f"{supermarket.get('name', '')}|{supermarket.get('address', '')}"
            
            if key not in seen:
                seen.add(key)
                unique_supermarkets.append(supermarket)
        
        removed = len(self.all_supermarkets) - len(unique_supermarkets)
        if removed > 0:
            print(f"â™»ï¸  {removed} ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯")
        
        self.all_supermarkets = unique_supermarkets
        return unique_supermarkets
    
    def save_to_json(self, filename: str = None) -> str:
        """
        Save data to JSON file
        
        Args:
            filename: Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ
            
        Returns:
            Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        """
        if not self.all_supermarkets:
            print("âš ï¸  Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return ""
        
        if not filename:
            # Ø³Ø§Ø®Øª Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø±
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            safe_city = self.city.replace(" ", "_")
            filename = f"supermarkets_{safe_city}_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.all_supermarkets, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
            return filename
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ JSON: {e}")
            return ""
    
    def get_statistics(self) -> Dict:
        """
        Get statistics about collected data
        """
        stats = {
            'total': len(self.all_supermarkets),
            'with_phone': 0,
            'with_location': 0,
            'with_address': 0
        }
        
        for supermarket in self.all_supermarkets:
            if supermarket.get('phone'):
                stats['with_phone'] += 1
            
            location = supermarket.get('location', {})
            if location.get('lat') and location.get('lon'):
                stats['with_location'] += 1
            
            if supermarket.get('address'):
                stats['with_address'] += 1
        
        return stats


# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª"""
    
    print("=" * 60)
    print("ğŸ›’ Balad.ir Supermarket Scraper")
    print("=" * 60)
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø´Ù‡Ø± Ø§Ø² Ú©Ø§Ø±Ø¨Ø±
    default_city = "ØªÙ‡Ø±Ø§Ù†"
    city_input = input(f"Ù†Ø§Ù… Ø´Ù‡Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: {default_city}): ").strip()
    city = city_input if city_input else default_city
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø±
    scraper = BaladSupermarketScraper(city=city, delay=1.5)
    
    # Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬
    try:
        supermarkets = scraper.scrape_all_pages(max_pages=50)  # Ø­Ø¯Ø§Ú©Ø«Ø± 50 ØµÙØ­Ù‡
        
        if not supermarkets:
            print("\nâŒ Ù‡ÛŒÚ† Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©ØªÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            print("Ø¹Ù„Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ:")
            print("1. Ù†Ø§Ù… Ø´Ù‡Ø± Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯")
            print("2. Ø¯Ø± Ø§ÛŒÙ† Ø´Ù‡Ø± Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©ØªÛŒ Ø«Ø¨Øª Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")
            print("3. Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§ÛŒØª ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª")
            return
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        unique_supermarkets = scraper.remove_duplicates()
        
        # Ø¢Ù…Ø§Ø±
        stats = scraper.get_statistics()
        print("\nğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {stats['total']}")
        print(f"   Ø¯Ø§Ø±Ø§ÛŒ ØªÙ„ÙÙ†: {stats['with_phone']}")
        print(f"   Ø¯Ø§Ø±Ø§ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ú©Ø§Ù†ÛŒ: {stats['with_location']}")
        print(f"   Ø¯Ø§Ø±Ø§ÛŒ Ø¢Ø¯Ø±Ø³: {stats['with_address']}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        filename = scraper.save_to_json()
        
        if filename:
            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            print("\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:")
            print("-" * 50)
            
            for i, market in enumerate(unique_supermarkets[:3], 1):
                print(f"\n{i}. {market.get('name', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
                print(f"   ğŸ“ ØªÙ„ÙÙ†: {market.get('phone', 'Ù†Ø¯Ø§Ø±Ø¯')}")
                print(f"   ğŸ“ Ø¢Ø¯Ø±Ø³: {market.get('address', 'Ù†Ø¯Ø§Ø±Ø¯')[:60]}...")
                loc = market.get('location', {})
                if loc.get('lat') and loc.get('lon'):
                    print(f"   ğŸ—ºï¸  Ù…ÙˆÙ‚Ø¹ÛŒØª: ({loc['lat']:.6f}, {loc['lon']:.6f})")
                else:
                    print(f"   ğŸ—ºï¸  Ù…ÙˆÙ‚Ø¹ÛŒØª: Ù†Ø¯Ø§Ø±Ø¯")
            
            print(f"\nâœ… ÙØ§ÛŒÙ„ JSON Ø´Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª: {filename}")
            print(f"ğŸ”§ Ø¨Ø±Ø§ÛŒ Ø´Ù‡Ø± Ø¯ÛŒÚ¯Ø± Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯: python {sys.argv[0]} Ø§ØµÙÙ‡Ø§Ù†")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        if scraper.all_supermarkets:
            save = input("Ø¢ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆÙ†Ø¯ØŸ (y/n): ").lower()
            if save == 'y':
                scraper.save_to_json()
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")


# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢Ø³Ø§Ù† Ø§Ø² Ø®Ø· ÙØ±Ù…Ø§Ù†
def quick_scrape(city: str, output_file: str = None):
    """
    Ø§Ø³Ú©Ø±Ù¾ Ø³Ø±ÛŒØ¹ ÛŒÚ© Ø´Ù‡Ø±
    
    Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡:
    >>> from balad_supermarket_scraper import quick_scrape
    >>> data = quick_scrape("Ø§ØµÙÙ‡Ø§Ù†", "supermarkets_isfahan.json")
    """
    scraper = BaladSupermarketScraper(city=city)
    data = scraper.scrape_all_pages(max_pages=30)
    data = scraper.remove_duplicates()
    
    if output_file:
        scraper.save_to_json(output_file)
    
    return data


if __name__ == "__main__":
    # Ø§Ú¯Ø± Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ø®Ø· ÙØ±Ù…Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
    if len(sys.argv) > 1:
        city_name = sys.argv[1]
        scraper = BaladSupermarketScraper(city=city_name)
        data = scraper.scrape_all_pages()
        scraper.remove_duplicates()
        
        filename = f"supermarkets_{city_name}.json"
        scraper.save_to_json(filename)
        
        print(f"\nâœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {city_name} Ú©Ø§Ù…Ù„ Ø´Ø¯. ÙØ§ÛŒÙ„: {filename}")
    else:
        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ
        main()