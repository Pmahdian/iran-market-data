"""
Balad.ir Supermarket Scraper - FIXED VERSION
Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø´Ù‡Ø± Ø§Ø² Ø³Ø§ÛŒØª Ø¨Ù„Ø¯
"""

import requests
import json
import time
from typing import List, Dict, Optional
import sys

class BaladSupermarketScraper:
  
    CITY_SLUGS = {
        "ØªÙ‡Ø±Ø§Ù†": "tehran",
        "Ø§ØµÙÙ‡Ø§Ù†": "esfahan",
        "Ù…Ø´Ù‡Ø¯": "mashhad",
        "Ø´ÛŒØ±Ø§Ø²": "shiraz",
        "ØªØ¨Ø±ÛŒØ²": "tabriz",
        "Ú©Ø±Ø¬": "karaj",
        "Ù‚Ù…": "qom",
        "Ø§Ù‡ÙˆØ§Ø²": "ahvaz",
        "Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡": "kermanshah",
        "Ø±Ø´Øª": "rasht",
        "Ø§Ø±ÙˆÙ…ÛŒÙ‡": "urmia",
        "ÛŒØ²Ø¯": "yazd",
        "Ù‡Ù…Ø¯Ø§Ù†": "hamedan",
        "Ø¨Ù†Ø¯Ø±Ø¹Ø¨Ø§Ø³": "bandar-abbas",
        "Ø§Ø±Ø§Ú©": "arak",
        "Ø²Ù†Ø¬Ø§Ù†": "zanjan",
        "Ù‚Ø²ÙˆÛŒÙ†": "qazvin",
        "Ø³Ù†Ù†Ø¯Ø¬": "sanandaj",
        "Ø³Ø§Ø±ÛŒ": "sari",
        "Ú¯Ø±Ú¯Ø§Ù†": "gorgan",
    }
    
    def __init__(self, city: str = "ØªÙ‡Ø±Ø§Ù†", delay: float = 1.5):
        """
        Initialize scraper for a specific city
        
        Args:
            city: Ù†Ø§Ù… Ø´Ù‡Ø± Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (Ù…Ø«Ù„Ø§Ù‹: ØªÙ‡Ø±Ø§Ù†ØŒ Ø§ØµÙÙ‡Ø§Ù†ØŒ tehran, esfahan)
            delay: ØªØ£Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)
        """
        self.original_city = city
        self.city_slug = self._get_city_slug(city)
        self.delay = delay
        self.base_url = f"https://balad.ir/city-{self.city_slug}/cat-supermarket"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fa-IR,fa;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        self.all_supermarkets = []
    
    def _get_city_slug(self, city: str) -> str:
      
        
        if city.lower() in self.CITY_SLUGS.values():
            return city.lower()
        
        # Ø§Ú¯Ø± Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø´Ù‡Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯
        if city in self.CITY_SLUGS:
            return self.CITY_SLUGS[city]
        
        # Ø§Ú¯Ø± Ø´Ù‡Ø± Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ø³Ø¹ÛŒ Ú©Ù† Ø­Ø¯Ø³ Ø¨Ø²Ù†ÛŒ
        print(f"âš ï¸  Ø´Ù‡Ø± '{city}' Ø¯Ø± Ù„ÛŒØ³Øª Ø´Ù‡Ø±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ù†ÛŒØ³Øª.")
        print("Ø´Ù‡Ø±Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡:")
        for persian, english in list(self.CITY_SLUGS.items())[:10]:
            print(f"  {persian} â†’ {english}")
        
        # Ø§Ø² Ú©Ø§Ø±Ø¨Ø± Ø¨Ø®ÙˆØ§Ù‡ slug Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†Ø¯
        user_slug = input(f"\nÙ„Ø·ÙØ§Ù‹ slug Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø´Ù‡Ø± '{city}' Ø±Ø§ Ø¯Ø± URL Ø¨Ù„Ø¯ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ").strip().lower()
        if user_slug:
            return user_slug
        
        # Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ØŒ ÙØ±Ø¶ Ú©Ù† Ù‡Ù…Ø§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø§Ø³Øª
        print(f"âš ï¸  Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² '{city}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† slug (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ø§Ø± Ù†Ú©Ù†Ø¯)")
        return city.lower()
    
    def fetch_page(self, page_number: int = 1) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© ØµÙØ­Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª"""
        try:
            # Ø³Ø§Ø®Øª URL
            if page_number == 1:
                url = self.base_url
            else:
                url = f"{self.base_url}?page={page_number}"
            
            print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ {page_number}: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=30)
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª HTTP
            if response.status_code == 404:
                print(f"âŒ ØµÙØ­Ù‡ {page_number} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ (404)")
                return None
            elif response.status_code != 200:
                print(f"âŒ Ø®Ø·Ø§ÛŒ HTTP {response.status_code} Ø¨Ø±Ø§ÛŒ ØµÙØ­Ù‡ {page_number}")
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            from bs4 import BeautifulSoup
            import re
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø±ÙˆØ´ Û±: Ø¬Ø³ØªØ¬ÙˆÛŒ __NEXT_DATA__
            next_data_script = soup.find('script', {'id': '__NEXT_DATA__'})
            
            if next_data_script:
                try:
                    data = json.loads(next_data_script.string)
                    items = self._extract_items_from_data(data)
                    
                    if items:
                        print(f"âœ… ØµÙØ­Ù‡ {page_number}: {len(items)} Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª ÛŒØ§ÙØª Ø´Ø¯")
                        
                        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ØµÙØ­Ø§Øª
                        page_count = 1
                        try:
                            page_count = data['props']['pageProps']['data']['pageCount']
                            print(f"ğŸ“– ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ØµÙØ­Ø§Øª: {page_count}")
                        except:
                            pass
                        
                        return {
                            'items': items,
                            'page_number': page_number,
                            'page_count': page_count,
                            'has_next': page_number < page_count if page_count > 1 else True
                        }
                except json.JSONDecodeError as e:
                    print(f"âŒ Ø®Ø·Ø§ÛŒ JSON Ø¯Ø± ØµÙØ­Ù‡ {page_number}: {e}")
            
            # Ø±ÙˆØ´ Û²: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ø³ØªÛŒ
            print(f"âš ï¸  ØµÙØ­Ù‡ {page_number}: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ† Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª Ø¯Ø± ØµÙØ­Ù‡
            if "Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª" in response.text:
                print(f"âœ… ØµÙØ­Ù‡ {page_number} Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª Ø§Ø³Øª")
                # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ regex Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒÙ…
                items = self._extract_with_regex(response.text)
                if items:
                    return {
                        'items': items,
                        'page_number': page_number,
                        'has_next': True
                    }
            
            print(f"âš ï¸  ØµÙØ­Ù‡ {page_number}: Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return None
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¯Ø± ØµÙØ­Ù‡ {page_number}: {e}")
            return None
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± ØµÙØ­Ù‡ {page_number}: {e}")
            return None
    
    def _extract_items_from_data(self, data: Dict) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡"""
        items = []
        
        try:
            # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§
            paths_to_try = [
                ['props', 'pageProps', 'data', 'items'],
                ['items'],
                ['data', 'items'],
            ]
            
            for path in paths_to_try:
                current = data
                found = True
                
                for key in path:
                    if isinstance(current, dict) and key in current:
                        current = current[key]
                    else:
                        found = False
                        break
                
                if found and isinstance(current, list) and len(current) > 0:
                    items = current
                    break
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
            standardized_items = []
            for item in items:
                standardized = self._standardize_item(item)
                if standardized:
                    standardized_items.append(standardized)
            
            return standardized_items
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§: {e}")
            return []
    
    def _extract_with_regex(self, html: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² regex (Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†)"""
        items = []
        
        try:
            # Ø§Ù„Ú¯Ùˆ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON Ø¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§
            import re
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ JSON-LD
            json_ld_pattern = r'<script type="application/ld\+json">(.*?)</script>'
            json_ld_matches = re.findall(json_ld_pattern, html, re.DOTALL)
            
            for match in json_ld_matches:
                try:
                    data = json.loads(match)
                    if isinstance(data, list):
                        for item in data:
                            standardized = self._standardize_item(item)
                            if standardized:
                                items.append(standardized)
                    elif isinstance(data, dict):
                        standardized = self._standardize_item(data)
                        if standardized:
                            items.append(standardized)
                except:
                    continue
            
            return items
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ regex: {e}")
            return []
    
    def _standardize_item(self, item: Dict) -> Optional[Dict]:
        """ØªØ¨Ø¯ÛŒÙ„ Ø¢ÛŒØªÙ… Ø®Ø§Ù… Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…
            name = item.get('name', '')
            if not name:
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ„ÙÙ†
            telephone = item.get('telephone', '') or item.get('phone', '')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø¯Ø±Ø³
            address = item.get('address', '')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ú©Ø§Ù†ÛŒ
            location = {'lat': None, 'lon': None}
            
            # Ø¨Ø±Ø±Ø³ÛŒ geometry Ø¨Ø±Ø§ÛŒ Ù…Ø®ØªØµØ§Øª
            geometry = item.get('geometry', {})
            if geometry and 'coordinates' in geometry:
                coords = geometry['coordinates']
                if len(coords) >= 2:
                    # ÙØ±Ù…Øª: [longitude, latitude]
                    location['lon'] = coords[0]
                    location['lat'] = coords[1]
            
            # Ø§Ú¯Ø± geometry Ù†Ø¨ÙˆØ¯ØŒ geo Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†
            if location['lat'] is None:
                geo = item.get('geo', {})
                if geo:
                    location['lat'] = geo.get('latitude')
                    location['lon'] = geo.get('longitude')
            
            # Ø³Ø§Ø®Øª Ø¢ÛŒØªÙ… Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
            standardized = {
                'name': name.strip(),
                'phone': str(telephone).strip(),
                'address': address.strip(),
                'location': location
            }
            
            return standardized
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¢ÛŒØªÙ…: {e}")
            return None
    
    def scrape_all_pages(self, max_pages: int = 50) -> List[Dict]:
        """Ø§Ø³Ú©Ø±Ù¾ ØªÙ…Ø§Ù… ØµÙØ­Ø§Øª Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª"""
        print(f"\nğŸ™ï¸  Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ {self.original_city} ({self.city_slug})")
        print("=" * 60)
        
        page_number = 1
        total_items = 0
        page_count = None
        
        while page_number <= max_pages:
            # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡
            page_data = self.fetch_page(page_number)
            
            if not page_data:
                print(f"\nâ¹ï¸  ØµÙØ­Ù‡ {page_number} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ Ø®Ø·Ø§ Ø¯Ø§Ø±Ø¯.")
                break
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§
            new_items = page_data.get('items', [])
            if new_items:
                self.all_supermarkets.extend(new_items)
                total_items += len(new_items)
                print(f"ğŸ“Š ØªØ§Ú©Ù†ÙˆÙ†: {total_items} Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª")
            
            # Ø°Ø®ÛŒØ±Ù‡ page_count Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            if 'page_count' in page_data and page_data['page_count']:
                page_count = page_data['page_count']
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¯Ø§Ù…Ù‡â€ŒØ¯Ø§Ø± Ø¨ÙˆØ¯Ù†
            if not page_data.get('has_next', True):
                print(f"\nâœ… Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† ØµÙØ­Ù‡ Ø±Ø³ÛŒØ¯ÛŒÙ….")
                break
            
            # Ø§Ú¯Ø± page_count Ù…Ø´Ø®Øµ Ø§Ø³Øª Ùˆ Ø¨Ù‡ Ø¢Ù† Ø±Ø³ÛŒØ¯Ù‡â€ŒØ§ÛŒÙ…
            if page_count and page_number >= page_count:
                print(f"\nâœ… ØªÙ…Ø§Ù… {page_count} ØµÙØ­Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")
                break
            
            # ØªØ£Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
            time.sleep(self.delay)
            
            # ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ
            page_number += 1
        
        print(f"\nğŸ‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ {self.original_city}: {len(self.all_supermarkets)}")
        
        return self.all_supermarkets
    
    def save_to_json(self, filename: str = None) -> str:
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
        if not self.all_supermarkets:
            print("âš ï¸  Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return ""
        
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            safe_city = self.city_slug
            filename = f"supermarkets_{safe_city}_{timestamp}.json"
        
        try:
            # ÙÙ‚Ø· ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
            filtered_data = []
            for item in self.all_supermarkets:
                filtered_item = {
                    'name': item.get('name', ''),
                    'phone': item.get('phone', ''),
                    'address': item.get('address', ''),
                    'location': item.get('location', {})
                }
                filtered_data.append(filtered_item)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
            
            # Ù†Ù…Ø§ÛŒØ´ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
            import os
            file_path = os.path.abspath(filename)
            print(f"ğŸ“ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„: {file_path}")
            
            return filename
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ JSON: {e}")
            return ""


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª"""
    print("=" * 60)
    print("ğŸ›’ Balad.ir Supermarket Scraper - FIXED")
    print("=" * 60)
    
    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§Ù‡Ù†Ù…Ø§
    print("\nğŸ“Œ Ø±Ø§Ù‡Ù†Ù…Ø§:")
    print("- Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø´Ù‡Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹: ØªÙ‡Ø±Ø§Ù†)")
    print("- ÛŒØ§ Ù†Ø§Ù… Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø´Ù‡Ø± Ø¯Ø± URL Ø¨Ù„Ø¯ (Ù…Ø«Ù„Ø§Ù‹: esfahan)")
    print("- Ø´Ù‡Ø±Ù‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡: ØªÙ‡Ø±Ø§Ù†ØŒ Ø§ØµÙÙ‡Ø§Ù†ØŒ Ù…Ø´Ù‡Ø¯ØŒ Ø´ÛŒØ±Ø§Ø²ØŒ ØªØ¨Ø±ÛŒØ²ØŒ ...")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø´Ù‡Ø± Ø§Ø² Ú©Ø§Ø±Ø¨Ø±
    default_city = "ØªÙ‡Ø±Ø§Ù†"
    city_input = input(f"\nÙ†Ø§Ù… Ø´Ù‡Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: {default_city}): ").strip()
    city = city_input if city_input else default_city
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø±
    try:
        scraper = BaladSupermarketScraper(city=city, delay=2.0)
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø±: {e}")
        return
    
    # Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬
    try:
        supermarkets = scraper.scrape_all_pages(max_pages=30)
        
        if not supermarkets:
            print(f"\nâŒ Ù‡ÛŒÚ† Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ù‡Ø± '{city}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            print("Ø¹Ù„Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ:")
            print("1. Ù†Ø§Ù… Ø´Ù‡Ø± Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯")
            print("2. Ø§ÛŒÙ† Ø´Ù‡Ø± Ø¯Ø± Ø¨Ù„Ø¯ Ø«Ø¨Øª Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")
            print("3. Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§ÛŒØª ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª")
            print("\nğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ù†Ø§Ù… Ø´Ù‡Ø± Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯")
            return
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        filename = scraper.save_to_json()
        
        if filename:
            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            print("\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:")
            print("-" * 50)
            
            for i, market in enumerate(supermarkets[:5], 1):
                print(f"\n{i}. {market.get('name', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
                print(f"   ğŸ“ ØªÙ„ÙÙ†: {market.get('phone', 'Ù†Ø¯Ø§Ø±Ø¯')}")
                print(f"   ğŸ“ Ø¢Ø¯Ø±Ø³: {market.get('address', 'Ù†Ø¯Ø§Ø±Ø¯')[:80]}...")
                loc = market.get('location', {})
                if loc.get('lat') and loc.get('lon'):
                    print(f"   ğŸ—ºï¸  Ù…ÙˆÙ‚Ø¹ÛŒØª: lat={loc['lat']:.6f}, lon={loc['lon']:.6f}")
                else:
                    print(f"   ğŸ—ºï¸  Ù…ÙˆÙ‚Ø¹ÛŒØª: Ù†Ø¯Ø§Ø±Ø¯")
            
            print(f"\nâœ… ÙØ§ÛŒÙ„ JSON Ø´Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª: {filename}")
            print(f"ğŸ”§ Ø¨Ø±Ø§ÛŒ Ø´Ù‡Ø± Ø¯ÛŒÚ¯Ø± Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯: python {sys.argv[0]} Ø§ØµÙÙ‡Ø§Ù†")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        if scraper.all_supermarkets:
            save = input("Ø¢ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆÙ†Ø¯ØŸ (y/n): ").lower()
            if save == 'y':
                scraper.save_to_json()
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")


# Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÛŒØ¹ Ø§Ø² Ø®Ø· ÙØ±Ù…Ø§Ù†
if __name__ == "__main__":
    # Ø§Ú¯Ø± Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ø®Ø· ÙØ±Ù…Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
    if len(sys.argv) > 1:
        city_name = sys.argv[1]
        scraper = BaladSupermarketScraper(city=city_name)
        data = scraper.scrape_all_pages(max_pages=30)
        
        if data:
            filename = f"supermarkets_{scraper.city_slug}.json"
            scraper.save_to_json(filename)
            print(f"\nâœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {city_name} Ú©Ø§Ù…Ù„ Ø´Ø¯. ÙØ§ÛŒÙ„: {filename}")
        else:
            print(f"\nâŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ù‡Ø± '{city_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯")
    else:
        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ
        main()