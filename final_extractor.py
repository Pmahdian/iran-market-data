# final_extractor.py
import json
import re
import requests
from bs4 import BeautifulSoup
import time

def extract_balad_supermarkets():
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ Ø§Ø² Ø¨Ù„Ø¯"""
    
    url = "https://balad.ir/city-tehran/cat-supermarket"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json, text/html, */*',
        'Accept-Language': 'fa-IR,fa;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://balad.ir/'
    }
    
    print("ğŸŒ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² Balad.ir...")
    
    try:
        # 1. Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        # 2. ÛŒØ§ÙØªÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON-LD
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø±ÙˆØ´ Û±: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² __NEXT_DATA__ (Ú©Ø§Ù…Ù„â€ŒØªØ±ÛŒÙ†)
        next_data_script = soup.find('script', {'id': '__NEXT_DATA__'})
        
        if next_data_script:
            print("âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ __NEXT_DATA__ ÛŒØ§ÙØª Ø´Ø¯")
            
            # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† JSON
            next_data = json.loads(next_data_script.string)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª
            items = next_data['props']['pageProps']['data']['items']
            
            print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(items)}")
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ù…Ø§
            supermarkets = []
            for item in items:
                supermarket = {
                    'name': item.get('name', 'Ù†Ø§Ù…Ø´Ø®Øµ'),
                    'phone': item.get('telephone', ''),
                    'address': item.get('address', ''),
                    'location': {
                        'lat': item.get('geometry', {}).get('coordinates', [None, None])[1],
                        'lon': item.get('geometry', {}).get('coordinates', [None, None])[0]
                    },
                    'rating': item.get('rating', {}).get('score'),
                    'rating_count': item.get('rating', {}).get('count'),
                    'website': item.get('website', ''),
                    'category': item.get('category', ''),
                    'token': item.get('token', '')
                }
                supermarkets.append(supermarket)
            
            return supermarkets
        
        else:
            print("âŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ __NEXT_DATA__ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return []
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡: {e}")
        return []

def extract_all_pages():
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ØªÙ…Ø§Ù… ØµÙØ­Ø§Øª (ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)"""
    all_supermarkets = []
    base_url = "https://balad.ir/city-tehran/cat-supermarket"
    
    # Ø§Ø¨ØªØ¯Ø§ ØµÙØ­Ù‡ Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ú¯ÛŒØ±ÛŒÙ…
    print("ğŸ” Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ Ø§ÙˆÙ„...")
    page1_supermarkets = extract_balad_supermarkets()
    all_supermarkets.extend(page1_supermarkets)
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ URL ØµÙØ­Ø§Øª Ø¨Ø¹Ø¯ÛŒ Ø¨Ù‡ Ø§ÛŒÙ† Ø´Ú©Ù„ Ø§Ø³Øª:
    # https://balad.ir/city-tehran/cat-supermarket?page=2
    
    # Ø¨Ø±Ø§ÛŒ ØªØ³ØªØŒ ÙÙ‚Ø· 3 ØµÙØ­Ù‡ Ø§ÙˆÙ„ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
    for page in range(2, 4):  # ØµÙØ­Ø§Øª 2 Ùˆ 3
        print(f"\nğŸ” Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ {page}...")
        page_url = f"{base_url}?page={page}"
        
        try:
            # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ù†Ø·Ù‚ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØµÙØ­Ø§Øª Ø¨Ø¹Ø¯ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ…
            # Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø³Ø§Ø®ØªØ§Ø± Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ø¯
            print(f"âš ï¸  Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙØ­Ø§Øª Ø¨Ø¹Ø¯ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ø¯")
            break  # ÙØ¹Ù„Ø§Ù‹ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØµÙØ­Ù‡ {page}: {e}")
            break
    
    return all_supermarkets

def save_to_json(data, filename):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return True
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
        return False

def main():
    print("=" * 60)
    print("Balad.ir Supermarket Data Extractor - FINAL")
    print("=" * 60)
    
    # Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§
    print("\nØ§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:")
    print("1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø· ØµÙØ­Ù‡ Ø§ÙˆÙ„ (20 Ø¢ÛŒØªÙ…)")
    print("2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ù†Ø¯ ØµÙØ­Ù‡ (Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ)")
    print("3. Ø®Ø±ÙˆØ¬")
    
    choice = input("\nÚ¯Ø²ÛŒÙ†Ù‡ Ø´Ù…Ø§ (1/2/3): ").strip()
    
    if choice == "1":
        supermarkets = extract_balad_supermarkets()
        
        if supermarkets:
            print(f"\nâœ… {len(supermarkets)} Ø³ÙˆÙ¾Ø±Ù…Ø§Ø±Ú©Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
            
            # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡
            print("\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:")
            for i, market in enumerate(supermarkets[:3], 1):
                print(f"\n{i}. {market['name']}")
                print(f"   ğŸ“ ØªÙ„ÙÙ†: {market['phone']}")
                print(f"   ğŸ“ Ø¢Ø¯Ø±Ø³: {market['address'][:50]}...")
                print(f"   ğŸ“ Ù…ÙˆÙ‚Ø¹ÛŒØª: lat={market['location']['lat']}, lon={market['location']['lon']}")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
            filename = f"supermarkets_tehran_{time.strftime('%Y%m%d_%H%M%S')}.json"
            save_to_json(supermarkets, filename)
            
            # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ CSV
            save_csv = input("\nØ¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¨Ù‡ CSV Ù‡Ù… Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆØ¯ØŸ (y/n): ").lower()
            if save_csv == 'y':
                save_to_csv(supermarkets, filename.replace('.json', '.csv'))
    
    elif choice == "2":
        print("\nâš ï¸  Ø§ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ Ø§Ø³Øª...")
        # supermarkets = extract_all_pages()
    
    else:
        print("\nØ®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ø±Ù†Ø§Ù…Ù‡...")

def save_to_csv(data, filename):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± CSV"""
    try:
        import csv
        
        # Ù†Ø§Ù… ÙÛŒÙ„Ø¯Ù‡Ø§
        fieldnames = ['name', 'phone', 'address', 'latitude', 'longitude', 'rating', 'rating_count', 'website', 'category']
        
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for item in data:
                row = {
                    'name': item['name'],
                    'phone': item['phone'],
                    'address': item['address'],
                    'latitude': item['location']['lat'],
                    'longitude': item['location']['lon'],
                    'rating': item['rating'],
                    'rating_count': item['rating_count'],
                    'website': item['website'],
                    'category': item['category']
                }
                writer.writerow(row)
        
        print(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return True
        
    except ImportError:
        print("âŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ csv Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ CSV: {e}")
        return False

# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡
def analyze_data_structure():
    """ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØªÛŒ"""
    url = "https://balad.ir/city-tehran/cat-supermarket"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù… Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ JSON-LD
    scripts = soup.find_all('script', {'type': 'application/ld+json'})
    
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ JSON-LD: {len(scripts)}")
    
    for i, script in enumerate(scripts):
        try:
            data = json.loads(script.string)
            print(f"\nğŸ”¹ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª {i+1}:")
            print(f"   Ù†ÙˆØ¹: {data.get('@type', 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡')}")
            print(f"   Ù†Ø§Ù…: {data.get('name', 'Ù†Ø¯Ø§Ø±Ø¯')}")
            
            if 'itemListElement' in data:
                print(f"   ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§: {len(data['itemListElement'])}")
            
        except:
            print(f"\nğŸ”¹ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª {i+1}: (Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù†)")

if __name__ == "__main__":
    # Ø§Ø¨ØªØ¯Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…
    print("ğŸ”¬ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
    analyze_data_structure()
    
    # Ø³Ù¾Ø³ Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ
    main()